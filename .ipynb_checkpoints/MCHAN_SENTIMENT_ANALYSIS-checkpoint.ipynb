{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0431f616-814c-42af-a300-66f632ecb42c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31292/644767591.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Set the spark context environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msqlContext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# Set the spark context environment \n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26ac2dd-0bad-4958-9d88-e1e166435f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "949b63c9-0fcc-4a75-bef7-c8a9d4fff4b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31292/3669502533.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreview_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"com.mongodb.spark.sql.DefaultSource\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m                    \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"uri\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"mongodb://ec2-34-212-28-18.us-west-2.compute.amazonaws.com/msan697.review\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                    \u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "reviews = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "                   .option(\"uri\",\"mongodb://ec2-34-212-28-18.us-west-2.compute.amazonaws.com/msan697.review\") \\\n",
    "                   .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c2f5ff-4eda-44cd-839a-c9034bcf06da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 10 records\n",
    "reviews.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddf39d7-932a-4bfa-bd49-44fb96c85ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of ratings across businesses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad6bcb-b4de-4b71-85c6-26a58eeca2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.groupBy(review_data[\"stars\"]) \\\n",
    "       .count() \\\n",
    "       .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e5302-9518-4558-99cf-e5a8a741f25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def positiveNegative(stars):\n",
    "    if stars <3:\n",
    "        return int(0) #negative reivews\n",
    "    elif stars >3 :\n",
    "        return int(1) #positive reviews\n",
    "    else:\n",
    "        return int(2) #neutral reviews \n",
    "    \n",
    "starsToSentiment = udf(lambda x:positiveNegative(x))\n",
    "\n",
    "trainTestRaw = review_data.select('text', starsToSentiment('stars') \\\n",
    "                               .alias('label')) \\\n",
    "                               .filter(\"label != 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1358985-1145-462a-9d67-875edd9118cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "trainTestRaw = trainTestRaw.withColumn(\"label\", train_test_DF_raw[\"label\"].cast(DoubleType()))\n",
    "\n",
    "trainTestRaw.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b6fa8-bf6f-4224-8f69-56cbc4065eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTestRaw.groupBy(train_test_DF[\"label\"]) \\\n",
    "            .count() \\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c3b7cd-c6a6-4daa-bef4-ffe289a16657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def removePunctuation(text):\n",
    "\n",
    "    my_string = text.replace(\"-\", \" \")\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "    nopunct = regex.sub(\" \", my_string) \n",
    "\n",
    "    nopunct = nopunct.split()\n",
    "    #nopunct = [stemmer.stem(w).strip(\" \") for w in nopunct] #remove stop word and normalize word using stemmer.\n",
    "    nopunct = [w.strip() for w in nopunct]\n",
    "    nopunct = ' '.join(nopunct)\n",
    "    \n",
    "    return nopunct\n",
    "\n",
    "udfNumPunct = udf(lambda x:removePunctuation(x))\n",
    "\n",
    "review_rmsw = train_test_DF.select(udfNumPunct('text').alias('text'), 'label')\n",
    "\n",
    "review_rmsw.show(1,truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a32174-e097-427f-9f29-b7a54fb37a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "remover= StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\").setCaseSensitive(False)\n",
    "hashingTF = HashingTF().setNumFeatures(n_features).setInputCol(\"filtered\").setOutputCol(\"rawFeatures\")\n",
    "idf = IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\").setMinDocFreq(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfe39fd-39ab-41ba-875b-a087ddf227f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set= review_rmsw.randomSplit([0.8, 0.2])\n",
    "train_set = train_set.cache()\n",
    "test_set = test_set.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd352315-d1d1-4b6e-8338-55d658b1fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy on the test set \n",
    "def evaluateMetric(predictions):\n",
    "    \n",
    "    evaluator = BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\")\n",
    "    print \"Area under ROC curve:\",evaluator.evaluate(predictions)\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                                  metricName=\"f1\")\n",
    "    f1 = evaluator.evaluate(predictions)\n",
    "    print(\"F1_score = %0.4f\" %(f1))\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                                  metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(\"Accuracy = %0.4f\" %(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da8e0b9-bbba-4418-9087-c79fd3ba480e",
   "metadata": {},
   "source": [
    "# Logistic Regresssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd68a7d-3291-4c54-b81b-ae14ff198894",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lr =  LogisticRegression(maxIter=100, regParam=0.01, elasticNetParam=0.8)\n",
    "pipeline = Pipeline(stages=[tokenizer,remover,hashingTF,idf, lr])\n",
    "logreg_model = pipeline.fit(train_set)\n",
    "\n",
    "lr_predictions = logreg_model.transform(test_set)\n",
    "\n",
    "# print the evaluation metrics\n",
    "evaluateMetric(lr_predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bb6868-41de-4a81-819a-f8d23fcd5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330305f1-972c-448e-99e1-a4e9a18d854d",
   "metadata": {},
   "source": [
    "# Unigram Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85731d9-be73-4044-812e-20d7e34b7001",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nb = NaiveBayes(smoothing = 1.0, modelType = \"multinomial\")\n",
    "pipeline=Pipeline(stages=[tokenizer,remover,hashingTF,idf, nb])\n",
    "nb_model=pipeline.fit(train_set)\n",
    "nb_predictions = nb_model.transform(test_set)\n",
    "\n",
    "# print evaluation metrics\n",
    "evaluateMetric(nb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656286eb-37d3-4052-89a9-45f766116656",
   "metadata": {},
   "source": [
    "# Bigram Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ca3541-a091-454a-a407-b1248b25a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "#remover= StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\").setCaseSensitive(False)\n",
    "bigram = NGram(n=2, inputCol=\"filtered\", outputCol=\"bigrams\")\n",
    "hashingTF_bigram = HashingTF().setNumFeatures(n_features).setInputCol(\"bigrams\").setOutputCol(\"rawFeatures\")\n",
    "idf_bigram = IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\").setMinDocFreq(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c275e41a-15d3-4e7e-95f1-887190c64a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nb = NaiveBayes(smoothing = 1.0, modelType = \"multinomial\")\n",
    "pipeline=Pipeline(stages=[tokenizer,remover,bigram,hashingTF_bigram,idf_bigram, nb])\n",
    "nb_model_bigram=pipeline.fit(train_set)\n",
    "\n",
    "nb_bigram_predictions = nb_model_bigram.transform(test_set)\n",
    "\n",
    "#print evaluation metrics\n",
    "evaluate_metric(nb_bigram_predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29abafd-b878-4998-a210-a128ad3a49d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_bigram_predictions.show(10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08535c8e-8aab-4a79-809f-46f5bdf73593",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
